version: '3.8'

services:
  # 1. FRONTEND
  frontend:
    build: ./frontend
    ports: ["3000:3000"]
    environment:
      - VITE_API_URL=http://localhost:8000
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - neural_net

  # 2. API GATEWAY
  api_gateway:
    build: ./backend/api_gateway
    ports: ["8000:8000"]
    environment:
      - AUTH_SERVICE_URL=http://auth_service:8002
      - CORTEX_URL=http://cortex:8000
      # FIX: Point to internal container port 8000
      - INFO_SERVICE_URL=http://info_service:8000
      # FIX: Added STT Service URL
      - STT_SERVICE_URL=http://stt_service:8003
    depends_on:
      - auth_service
      - cortex
      - info_service
      - stt_service
    networks:
      - neural_net

  # 3. AUTH SERVICE
  auth_service:
    build: ./backend/auth_service
    ports: ["8002:8002"]
    volumes:
      - ./backend/auth_service:/app
      - ./workspace_data:/app/workspace
    networks:
      - neural_net

  # 4. CORTEX ORCHESTRATOR
  cortex:
    build: ./backend/cortex
    ports: ["8008:8000"] 
    environment:
      - LOG_LEVEL=INFO
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379/0
      - LLM_SERVICE_URL=http://llm_service:8004
      - TTS_SERVICE_URL=http://tts_service:8001
      - STT_SERVICE_URL=http://stt_service:8003
      - FINANCE_SERVICE_URL=http://finance_service:8006
      - AUTOMATION_SERVICE_URL=http://automation_service:8005
      - HF_HUB_DOWNLOAD_TIMEOUT=300   # 5 minutes
      - HF_HUB_ETAG_TIMEOUT=300
      - HF_HOME=/app/data/hf_cache    # Standard HF cache location
      - FASTEMBED_CACHE_PATH=/app/data/fastembed_cache
    depends_on:
      - qdrant
      - redis
      - llm_service
      - tts_service
      - finance_service
    volumes:
      - cortex_data:/app/data
    networks:
      - neural_net

  # 5. STT SERVICE (API)
  stt_service:
    build: ./backend/stt_service
    ports: ["8003:8003"]
    environment:
      - REDIS_URL=redis://redis:6379/0
      - SHARED_VOL=/app/shared_data
    volumes:
      - stt_shared_data:/app/shared_data
    depends_on:
      - redis
    networks:
      - neural_net

  # 6. STT WORKER (GPU - Faster Whisper)
  stt_worker:
    build: ./backend/stt_service
    # Concurrency 1, GPU memory safe mode
    command: celery -A celery_stt worker --loglevel=info --concurrency=1 -Q stt_queue
    environment:
      - REDIS_URL=redis://redis:6379/0
      - SHARED_VOL=/app/shared_data
      - WHISPER_MODEL_SIZE=small 
      - WHISPER_DEVICE=cuda
      - COMPUTE_TYPE=float16
    volumes:
      - stt_shared_data:/app/shared_data
      - whisper_models:/opt/whisper_models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - redis
    networks:
      - neural_net

  # 7. LLM SERVICE (Client for Ollama)
  llm_service:
    build: ./backend/llm_service
    ports: ["8004:8004"]
    environment:
      - OLLAMA_URL=http://ollama:11434
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=INFO
    depends_on:
      - ollama
      - redis
    networks:
      - neural_net

  # 8. TTS SERVICE (Kokoro - CPU Optimized)
  tts_service:
    build: ./backend/tts_service
    ports: ["8001:8001"]
    environment:
      - LOG_LEVEL=INFO
      # CPU is MANDATORY here to save VRAM for LLM+STT
      - DEVICE=cpu 
      - USE_ONNX=true
      - MODEL_PATH=/opt/neural_models/kokoro-v1.0.onnx
      - VOICES_PATH=/opt/neural_models/voices-v1.0.bin
    # volumes:
    #   - tts_models:/opt/neural_models
    networks:
      - neural_net

  # 9. FINANCE SERVICE (Active but Idle)
  finance_service:
    build: ./backend/finance_service
    ports: ["8006:8006"]
    environment:
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite:///./finance.db
      - LOG_LEVEL=INFO
    depends_on:
      - redis
    volumes:
      - ./backend/finance_service:/app
      - finance_data:/app/data
    networks:
      - neural_net

  # 10. AUTOMATION SERVICE (Active but Idle)
  automation_service:
    build: ./backend/automation_service
    ports: ["8005:8005"]
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./backend/automation_service:/app
    networks:
      - neural_net

  # 11. CELERY WORKER (General Automation - CPU Only)
  celery_worker:
    build: ./backend/automation_service
    command: celery -A celery_app worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - automation_service
    volumes:
      - ./backend/automation_service:/app
    networks:
      - neural_net

  # 12. QDRANT (Vector Database)
  qdrant:
    image: qdrant/qdrant:latest
    ports: ["6333:6333"]
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - neural_net

  # 13. OLLAMA ENGINE (GPU - Llama 3.2 1B)
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes:
      - ollama_storage:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=-1 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - neural_net

  # 14. REDIS
  redis:
    image: redis:alpine
    ports: ["6379:6379"]
    volumes:
      - redis_data:/data
    networks:
      - neural_net
      
  # 15. INFO SERVICE
  info_service:
    build: ./backend/info_service
    # FIX: Host 8007 maps to Container 8000
    ports: ["8007:8000"]
    environment:
      - LOG_LEVEL=INFO
    networks:
      - neural_net

networks:
  neural_net:
    driver: bridge

volumes:
  ollama_storage:
  tts_models:
  whisper_models:
  redis_data:
  stt_shared_data:
  finance_data:
  qdrant_storage:
  cortex_data: