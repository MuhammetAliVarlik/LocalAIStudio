version: '3.8'

services:
  # 1. FRONTEND
  frontend:
    build: ./frontend
    ports: ["3000:3000"]
    environment:
      - VITE_API_URL=http://localhost:8000
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - neural_net

  # 2. API GATEWAY (Public Facing Entry Point)
  # Tüm trafik buradan geçer. Auth ve Cortex'e yönlendirir.
  api_gateway:
    build: ./backend/api_gateway
    ports: ["8000:8000"]
    environment:
      - AUTH_SERVICE_URL=http://auth_service:8002
      - CORTEX_URL=http://cortex:8000
    depends_on:
      - auth_service
      - cortex
    networks:
      - neural_net

  # 3. AUTH SERVICE
  auth_service:
    build: ./backend/auth_service
    ports: ["8002:8002"]
    volumes:
      - ./backend/auth_service:/app
      - ./workspace_data:/app/workspace # Persist DB
    networks:
      - neural_net

  # 4. CORTEX ORCHESTRATOR (The Brain)
  # Diğer tüm servisleri yönetir.
  cortex:
    build: ./backend/cortex
    # Host portunu 8008 yaptık (Gateway 8000 ile çakışmasın diye).
    # Docker içi port 8000'dir.
    ports: ["8008:8000"] 
    environment:
      - LOG_LEVEL=INFO
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379/0
      # Service Discovery URLs (Internal)
      - LLM_SERVICE_URL=http://llm_service:8004
      - TTS_SERVICE_URL=http://tts_service:8001
      - STT_SERVICE_URL=http://stt_service:8003
      - FINANCE_SERVICE_URL=http://finance_service:8006
    depends_on:
      - qdrant
      - redis
      - llm_service
      - tts_service
      - finance_service
    networks:
      - neural_net

  # 5. STT SERVICE (API - Producer)
  stt_service:
    build: ./backend/stt_service
    ports: ["8003:8003"]
    environment:
      - REDIS_URL=redis://redis:6379/0
      - SHARED_VOL=/app/shared_data
    volumes:
      - stt_shared_data:/app/shared_data
    depends_on:
      - redis
    networks:
      - neural_net

  # 6. STT WORKER (GPU - Consumer)
  stt_worker:
    build: ./backend/stt_service
    command: celery -A celery_stt worker --loglevel=info --concurrency=1 -Q stt_queue
    environment:
      - REDIS_URL=redis://redis:6379/0
      - SHARED_VOL=/app/shared_data
      - WHISPER_MODEL_SIZE=base
      - WHISPER_DEVICE=cuda
    volumes:
      - stt_shared_data:/app/shared_data
      - whisper_models:/opt/whisper_models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - redis
    networks:
      - neural_net

  # 7. LLM SERVICE
  llm_service:
    build: ./backend/llm_service
    ports: ["8004:8004"]
    environment:
      - OLLAMA_URL=http://ollama:11434
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=INFO
    depends_on:
      - ollama
      - redis
    networks:
      - neural_net

  # 8. TTS SERVICE
  tts_service:
    build: ./backend/tts_service
    ports: ["8001:8001"]
    environment:
      - LOG_LEVEL=INFO
      - DEVICE=cpu
      - USE_ONNX=true
    volumes:
      - tts_models:/opt/neural_models
    networks:
      - neural_net

  # 9. FINANCE SERVICE
  finance_service:
    build: ./backend/finance_service
    ports: ["8006:8006"]
    environment:
      - REDIS_URL=redis://redis:6379/0
      - DATABASE_URL=sqlite:///./finance.db
      - LOG_LEVEL=INFO
    depends_on:
      - redis
    volumes:
      - ./backend/finance_service:/app
      - finance_data:/app/data
    networks:
      - neural_net

  # 10. AUTOMATION SERVICE (API)
  automation_service:
    build: ./backend/automation_service
    ports: ["8005:8005"]
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    volumes:
      - ./backend/automation_service:/app
    networks:
      - neural_net

  # 11. CELERY WORKER (General Automation)
  celery_worker:
    build: ./backend/automation_service
    command: celery -A celery_app worker --loglevel=info
    environment:
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - automation_service
    volumes:
      - ./backend/automation_service:/app
    networks:
      - neural_net

  # 12. QDRANT (Vector Database)
  qdrant:
    image: qdrant/qdrant:latest
    ports: ["6333:6333"]
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - neural_net

  # 13. OLLAMA ENGINE
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes:
      - ollama_storage:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=-1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - neural_net

  # 14. REDIS (Message Broker)
  redis:
    image: redis:alpine
    ports: ["6379:6379"]
    volumes:
      - redis_data:/data
    networks:
      - neural_net
      
  # 15. INFO SERVICE (System Monitor & News)
  info_service:
    build: ./backend/info_service
    ports: ["8007:8007"]
    environment:
      - LOG_LEVEL=INFO
    networks:
      - neural_net

networks:
  neural_net:
    driver: bridge

volumes:
  ollama_storage:
  tts_models:
  whisper_models:
  redis_data:
  stt_shared_data:
  finance_data:
  qdrant_storage: